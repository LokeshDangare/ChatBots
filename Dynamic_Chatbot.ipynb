{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oui0yZX0oOAC"
      },
      "source": [
        "### Importing all the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMoEyOOYpLVg",
        "outputId": "25747940-258f-4ed7-9627-e8ec87417f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=8cef1d577c9890c0aee6c964ad4f4ee662825ff0b9771b54d51eecad6934da7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Acsyz59angkY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import string #Punctuation, data preprocessing\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # data encoding\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-geqgskToaZY",
        "outputId": "262527c8-d3f4-4f40-aaa9-d965d910e625"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqBpVUVJohOb"
      },
      "source": [
        "## Tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qI_L75T1ofGG"
      },
      "outputs": [],
      "source": [
        "def tokenize(user_query):\n",
        "    corpus = wikipedia.summary(user_query)\n",
        "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
        "    word_tokens = nltk.word_tokenize(corpus)\n",
        "    return sentence_tokens, word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxw4jA-qp8p2"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hHD1IR06pmMP"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EQ68MhH8qBvo"
      },
      "outputs": [],
      "source": [
        "def lemtokens(tokens):\n",
        "    list = []\n",
        "    for i in tokens: #Every individual token have to be lemmatized.\n",
        "        list.append(lemmatizer.lemmatize(i))\n",
        "    return list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P1wfBLTZqF_n",
        "outputId": "21ca8282-09e5-43f2-ef48-ccf65e95c7c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_m12HnuoqI5v"
      },
      "outputs": [],
      "source": [
        "# Remove the Punctuation.\n",
        "punct_dict = dict((ord(i), None) for i in string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ngvX5irKqPnC"
      },
      "outputs": [],
      "source": [
        "def lemmer(text):\n",
        "    tokenized_text = nltk.word_tokenize(text.lower().translate(punct_dict))\n",
        "    lemmatize_values = lemtokens(tokenized_text)\n",
        "    return lemmatize_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZSrjnfvqTG9",
        "outputId": "bd449086-d11b-4a77-b6b3-79b2c606b11b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNkGnSG4qia4"
      },
      "source": [
        "#### Function for Greeting: Rule Based.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tm413CQ8rb52"
      },
      "outputs": [],
      "source": [
        "greeting_inputs = ['hello', 'hi', 'hey', 'greeting']\n",
        "greeting_responses = ['I am a chatbot', 'hi', 'hey', 'hello', 'whats up']\n",
        "\n",
        "def greeting(text):\n",
        "    for tokens in text.split():\n",
        "        if tokens.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_hUxYRFrfbE"
      },
      "source": [
        "#### Function for generating responses for queries from the corpus.\n",
        "* Data Encoding - TF-IDF\n",
        "* Similarity Metrics - Cosine Similarity.\n",
        "* Choosing vector with maximum similarity in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vM8yrFp-qWNZ"
      },
      "outputs": [],
      "source": [
        "def respond(user_query):\n",
        "    bot_response = ''\n",
        "\n",
        "    #Tokenize\n",
        "    sent_tokens, word_tokens = tokenize(user_query)\n",
        "    sent_tokens.append(user_query)\n",
        "\n",
        "    # Vectorizing\n",
        "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer, stop_words=\"english\")\n",
        "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
        "\n",
        "    #Cosine Similarity\n",
        "    sim_values = cosine_similarity(tfidf[-1], tfidf) #Cosine similarity of the last element with entire list\n",
        "\n",
        "    # Selecting response or tokens with max similarity\n",
        "    index = sim_values.argsort()[0][-2]\n",
        "\n",
        "    flatten_sim = sim_values.flatten()\n",
        "    flatten_sim.sort()\n",
        "\n",
        "    required_tfidf = flatten_sim[-2]\n",
        "\n",
        "    if(required_tfidf == 0):\n",
        "        bot_response += 'I cannot understand.'\n",
        "        return bot_response\n",
        "    else:\n",
        "        bot_response += sent_tokens[index]\n",
        "        return bot_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R78imkTcrRE8",
        "outputId": "7934eff7-7355-4baf-e8ab-bc82faba3719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CHATBOT\n",
            "HI\n",
            "Chatbot: hello\n",
            "apple fruit\n",
            "Chatbot:  An apple is a round, edible fruit produced by an apple tree (Malus spp., among them the domestic or orchard apple; Malus domestica).\n",
            "Who is the winner of mtv hustle season 3\n",
            "Chatbot:  On 10 December 2023, during episode 16 of season 3, Samantha Ruth Prabhu announced the Tamil edition of MTV Hustle Namma Pettai.\n",
            "who was the winner of MTV Hustle show.\n",
            "Chatbot:  MTV Hustle is an Indian rap and hip-hop reality show.\n",
            "who is the king of fruits\n",
            "Chatbot:  Takaya began a sequel titled Fruits Basket Another in September 2015, and the spin-off series The Three Musketeers Arc in April 2019.\n",
            "exit\n",
            "Chatbot: Bye! Have a good day ahead.\n"
          ]
        }
      ],
      "source": [
        "print(\"CHATBOT\")\n",
        "flag = 1\n",
        "\n",
        "while(flag == 1):\n",
        "    user_query = input()\n",
        "    user_query = user_query.lower()\n",
        "\n",
        "    if(user_query=='exit'):\n",
        "        flag = 0\n",
        "        print(\"Chatbot: Bye! Have a good day ahead.\")\n",
        "\n",
        "    else:\n",
        "        #greeting\n",
        "        if(greeting(user_query) != None):\n",
        "            print(\"Chatbot: \"+ greeting(user_query))\n",
        "\n",
        "        else:\n",
        "            res = respond(user_query)\n",
        "            print(\"Chatbot: \", res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHmEZ2iJrh44"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
