{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing all the Libraries.\n"
      ],
      "metadata": {
        "id": "ynOparDMJ6pK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c1VIr3OZJkqI"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import string #Punctuation, data preprocessing\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # data encoding\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WTPs2xAKaVG",
        "outputId": "51e035dd-6b7d-43ab-c83e-02adaa9b7180"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization."
      ],
      "metadata": {
        "id": "OwRGDt_aKIaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize():\n",
        "    file = open('corpus.txt', 'r', errors='ignore')\n",
        "    corpus = file.read()\n",
        "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
        "    word_tokens = nltk.word_tokenize(corpus)\n",
        "    return sentence_tokens, word_tokens"
      ],
      "metadata": {
        "id": "8tdys_8EKEL9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDP_z-6_KPj-",
        "outputId": "05762afa-3669-48d5-94a1-f88d3417a873"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Artificial intelligence is a field of science concerned with building computers and machines that can reason, learn, and act in such a way that would normally require human intelligence or that involves data whose scale exceeds what humans can analyze.',\n",
              "  'AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology.',\n",
              "  'On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more.'],\n",
              " ['Artificial',\n",
              "  'intelligence',\n",
              "  'is',\n",
              "  'a',\n",
              "  'field',\n",
              "  'of',\n",
              "  'science',\n",
              "  'concerned',\n",
              "  'with',\n",
              "  'building',\n",
              "  'computers',\n",
              "  'and',\n",
              "  'machines',\n",
              "  'that',\n",
              "  'can',\n",
              "  'reason',\n",
              "  ',',\n",
              "  'learn',\n",
              "  ',',\n",
              "  'and',\n",
              "  'act',\n",
              "  'in',\n",
              "  'such',\n",
              "  'a',\n",
              "  'way',\n",
              "  'that',\n",
              "  'would',\n",
              "  'normally',\n",
              "  'require',\n",
              "  'human',\n",
              "  'intelligence',\n",
              "  'or',\n",
              "  'that',\n",
              "  'involves',\n",
              "  'data',\n",
              "  'whose',\n",
              "  'scale',\n",
              "  'exceeds',\n",
              "  'what',\n",
              "  'humans',\n",
              "  'can',\n",
              "  'analyze',\n",
              "  '.',\n",
              "  'AI',\n",
              "  'is',\n",
              "  'a',\n",
              "  'broad',\n",
              "  'field',\n",
              "  'that',\n",
              "  'encompasses',\n",
              "  'many',\n",
              "  'different',\n",
              "  'disciplines',\n",
              "  ',',\n",
              "  'including',\n",
              "  'computer',\n",
              "  'science',\n",
              "  ',',\n",
              "  'data',\n",
              "  'analytics',\n",
              "  'and',\n",
              "  'statistics',\n",
              "  ',',\n",
              "  'hardware',\n",
              "  'and',\n",
              "  'software',\n",
              "  'engineering',\n",
              "  ',',\n",
              "  'linguistics',\n",
              "  ',',\n",
              "  'neuroscience',\n",
              "  ',',\n",
              "  'and',\n",
              "  'even',\n",
              "  'philosophy',\n",
              "  'and',\n",
              "  'psychology',\n",
              "  '.',\n",
              "  'On',\n",
              "  'an',\n",
              "  'operational',\n",
              "  'level',\n",
              "  'for',\n",
              "  'business',\n",
              "  'use',\n",
              "  ',',\n",
              "  'AI',\n",
              "  'is',\n",
              "  'a',\n",
              "  'set',\n",
              "  'of',\n",
              "  'technologies',\n",
              "  'that',\n",
              "  'are',\n",
              "  'based',\n",
              "  'primarily',\n",
              "  'on',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'and',\n",
              "  'deep',\n",
              "  'learning',\n",
              "  ',',\n",
              "  'used',\n",
              "  'for',\n",
              "  'data',\n",
              "  'analytics',\n",
              "  ',',\n",
              "  'predictions',\n",
              "  'and',\n",
              "  'forecasting',\n",
              "  ',',\n",
              "  'object',\n",
              "  'categorization',\n",
              "  ',',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  ',',\n",
              "  'recommendations',\n",
              "  ',',\n",
              "  'intelligent',\n",
              "  'data',\n",
              "  'retrieval',\n",
              "  ',',\n",
              "  'and',\n",
              "  'more',\n",
              "  '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "dOg965HZKtfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "nBVUIqj-KToD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemtokens(tokens):\n",
        "    list = []\n",
        "    for i in tokens: #Every individual token have to be lemmatized.\n",
        "        list.append(lemmatizer.lemmatize(i))\n",
        "    return list\n"
      ],
      "metadata": {
        "id": "QhlaQ4eELTGy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pSGwDIk0MH2o",
        "outputId": "ca8962ca-1c8d-4c3f-d8bd-d6e63099643b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the Punctuation.\n",
        "punct_dict = dict((ord(i), None) for i in string.punctuation)"
      ],
      "metadata": {
        "id": "C-Fyf3sKL0LB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#punct_dict"
      ],
      "metadata": {
        "id": "mcbjmSaXMZlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chr(33)"
      ],
      "metadata": {
        "id": "0pjIwd5iMbPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmer(text):\n",
        "    tokenized_text = nltk.word_tokenize(text.lower().translate(punct_dict))\n",
        "    lemmatize_values = lemtokens(tokenized_text)\n",
        "    return lemmatize_values"
      ],
      "metadata": {
        "id": "zSHbvYh9MiRv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7XW6XgLOfND",
        "outputId": "272b56e5-9a98-46a9-eca2-2f0828003f2a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer('Ma@ngo is the king of fruits!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu6J9IBGOLrv",
        "outputId": "ea62ba7c-8f10-4d9f-9643-acbf70194e96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mango', 'is', 'the', 'king', 'of', 'fruit']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for Greeting: Rule based."
      ],
      "metadata": {
        "id": "yfNNFW5XdW_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greeting_inputs = ['hello', 'hi', 'hey', 'greeting']\n",
        "greeting_responses = ['I am a chatbot', 'hi', 'hey', 'hello', 'whats up']\n",
        "\n",
        "def greeting(text):\n",
        "    for tokens in text.split():\n",
        "        if tokens.lower() in greeting_inputs:\n",
        "            return random.choice(greeting_responses)"
      ],
      "metadata": {
        "id": "X_5dNVnSOcJM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for generating responses for queries from the corpus.\n",
        "* Data Encoding - TF-IDF\n",
        "* Similarity Metrics - Cosine Similarity.\n",
        "* Choosing vector with maximum similarity in the corpus."
      ],
      "metadata": {
        "id": "Ee7sJ6UTeYBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(user_query):\n",
        "    bot_response = ''\n",
        "\n",
        "    #Tokenize\n",
        "    sent_tokens, word_tokens = tokenize()\n",
        "    sent_tokens.append(user_query)\n",
        "\n",
        "    # Vectorizing\n",
        "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer, stop_words=\"english\")\n",
        "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
        "\n",
        "    #Cosine Similarity\n",
        "    sim_values = cosine_similarity(tfidf[-1], tfidf) #Cosine similarity of the last element with entire list\n",
        "\n",
        "    # Selecting response or tokens with max similarity\n",
        "    index = sim_values.argsort()[0][-2]\n",
        "\n",
        "    flatten_sim = sim_values.flatten()\n",
        "    flatten_sim.sort()\n",
        "\n",
        "    required_tfidf = flatten_sim[-2]\n",
        "\n",
        "    if(required_tfidf == 0):\n",
        "        bot_response += 'I cannot understand.'\n",
        "        return bot_response\n",
        "    else:\n",
        "        bot_response += sent_tokens[index]\n",
        "        return bot_response"
      ],
      "metadata": {
        "id": "ptCZdaxZeS-3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write main function for chatbot."
      ],
      "metadata": {
        "id": "UAK1g-4fkaS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CHATBOT\")\n",
        "flag = 1\n",
        "\n",
        "while(flag == 1):\n",
        "    user_query = input()\n",
        "    user_query = user_query.lower()\n",
        "\n",
        "    if(user_query=='exit'):\n",
        "        flag = 0\n",
        "        print(\"Chatbot: Bye! Have a good day ahead.\")\n",
        "\n",
        "    else:\n",
        "        #greeting\n",
        "        if(greeting(user_query) != None):\n",
        "            print(\"Chatbot: \"+ greeting(user_query))\n",
        "\n",
        "        else:\n",
        "            res = respond(user_query)\n",
        "            print(\"Chatbot: \", res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YPfZa2Mj7kJ",
        "outputId": "31e08c5d-2320-4e5c-ebf2-b16343e45572"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHATBOT\n",
            "hi\n",
            "Chatbot: hey\n",
            "what is deep learning\n",
            "Chatbot:  On an operational level for business use, AI is a set of technologies that are based primarily on machine learning and deep learning, used for data analytics, predictions and forecasting, object categorization, natural language processing, recommendations, intelligent data retrieval, and more.\n",
            "what is AI\n",
            "Chatbot:  AI is a broad field that encompasses many different disciplines, including computer science, data analytics and statistics, hardware and software engineering, linguistics, neuroscience, and even philosophy and psychology.\n",
            "exit\n",
            "Chatbot: Bye! Have a good day ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmYFbx64mZyO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}